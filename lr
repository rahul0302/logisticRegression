{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Logistic Regression: Basic Neural Network\n",
    "\n",
    "The idea behind improving logistic regression is to use additional hidden layers in between the input and the output.\n",
    "\n",
    "![Fig 1: Baisc Logistic Regression](images/Neural1.png)\n",
    "![Fig 2: Basic NeuralNetwork](images/Network.png)\n",
    "\n",
    "Each layer is composed of weights that are passed through activation function that produces an output.\n",
    "\n",
    "## Input or Visible Layers\n",
    "\n",
    "The bottom layer take sthe input from our dataset and in this case just pass the input value through the next layer\n",
    "\n",
    "## Hidden layers\n",
    "\n",
    "I decided for this model that I used two hidden layers that use sigmoid and the rectifier function as their activation functions. This was after some experimentation that I was able to determine waht would be the best with only two hidden layers.\n",
    "\n",
    "In addition I also decided to add a dropout layer before caluculating the output neuron in order to prevent overfitting in our data. \n",
    "\n",
    "## Output Layer\n",
    "\n",
    "Our output layer will be responsible for outputting a value that correspodns to the format required for the problem. In this case, we will have a 10 output neurons that uses a softmax activation function to output one of 10 possible classes for our specific dataset. THe output with the highest probability can be used to produce a classification value for the MNIST data set. \n",
    "\n",
    "Before starting the model we first needed to define some parameters and load the MNIST dataset. learning_rate will be the incremental update for our optimizer, and batch_size will be the numer of images we train at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Starter code for logistic regression model\n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 1000\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('/data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we begin to define our layers and set our weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9.\n",
    "# Features are of the type float, and labels are of the type int\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='X_placeholder')  # mnist data is 784 for each image\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='Y_placeholder')\n",
    "\n",
    "\n",
    "with tf.name_scope('hidden_1'):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [784, 256], stddev=0.01), name='hidden_w1')\n",
    "    b = tf.Variable(tf.zeros([256]), name='hidden_b2')\n",
    "    with tf.name_scope('relu'):\n",
    "        layer1 = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "\n",
    "with tf.name_scope('hidden_2'):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "    w_1 = tf.Variable(tf.truncated_normal(\n",
    "        [256, 256], stddev=0.01), name='hidden_w2')\n",
    "    b_1 = tf.Variable(tf.zeros([256]), name='hidden_b2')\n",
    "    with tf.name_scope('sigmoid'):\n",
    "        layer2 = tf.nn.sigmoid(tf.matmul(layer1, w_1) + b_1)\n",
    "\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "    dropped = tf.nn.dropout(layer2, 0.75, name='dropout')\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "\n",
    "    w_2 = tf.Variable(tf.truncated_normal(\n",
    "        [256, 10], stddev=0.01), name='output_w')\n",
    "    b_2 = tf.Variable(tf.zeros([10]), name='output_b')\n",
    "    logits = tf.matmul(dropped, w_2) + b_2\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    tf.summary.scalar('loss', cost)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network Model\n",
    "\n",
    "As we see in our previous model, in order to improve upon Logistic Regression, in this example we will create a simple neural network using the softmax regression as a conversion for probabilities after having built up our input. The general format of our model will look like this image.\n",
    "\n",
    "We will use the AdamOptimizer. It is an optimizer that attempts to use the average of moements of the gradients and calulates an exponential moving average fo the gradient and the squared gradient, with two parameters controlling the decay rates of moving averages. It also has built in bias correction by calculating biased estimates as well as bias-corrected estimates. Empirically, they have demonstrated good results in showing convergence in fewer iterations (Adam: A Method for Stochastic Optimization https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opti = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    writer = tf.summary.FileWriter('./graphs/neuralnet', sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_batches = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(n_epochs):  # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # TO-DO: run optimizer + fetch loss\n",
    "            _, loss_batch, summ_batch = sess.run([opti, cost, summary_op], feed_dict={\n",
    "                x: X_batch, y: Y_batch})\n",
    "            writer.add_summary(summ_batch, i)\n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss / n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!')  # should be around 0.35 after 25 epochs\n",
    "\n",
    "    # test the model\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # need numpy.count_nonzero(boolarr) :(\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    n_batches = int(mnist.test.num_examples / batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run([accuracy], feed_dict={\n",
    "            x: X_batch, y: Y_batch})\n",
    "        total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_preds / mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is how we trained our model using the described optimizer. We also added logging to every 100th iteration in the training process to understand how our training accuracy and average loss is going. DROPOUT is the parameter used to control the dropout rate. Here are the plots that were calculated from fully trained the model.\n",
    "\n",
    "\n",
    "![Fig 2: Graph of Network Model](images/nn_better.png)\n",
    "\n",
    "\n",
    "![Fig 3: Training Accuracy](images/NeuralNet1.png)\n",
    "\n",
    "\n",
    "![Fig 4: Loss Function](images/NeuralNet2.png)\n",
    "\n",
    "\n",
    "Our final result was around 97.1%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
